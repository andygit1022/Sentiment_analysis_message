{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # True라면 GPU 사용 가능\n",
    "print(torch.cuda.get_device_name(0))  # GPU의 이름 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>previous_text</th>\n",
       "      <th>current_text</th>\n",
       "      <th>sentiments</th>\n",
       "      <th>situation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>엄마 아기가 태어나니까 내가 부모로서 해야 할 게 참 많은 것 같아요.</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그렇지? 아기 키우는 게 여간 어려운 일이 아니야.</td>\n",
       "      <td>어제 평소보다도 격하게 막 온몸을 써가면서 울더라고요. 얼마나 당황했는지 몰라요.</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>배가 고파서 그랬던 것 아닐까? 아기들은 배가 고프면 몸부림을 친단다.</td>\n",
       "      <td>맞아요. 젖을 물려주니 금세 뚝 그쳤어요. 난 분명히 순했을 것 같은데.</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>말도 마. 네가 얼마나 까탈스러웠는데. 우리 손주가 내 딸을 빼다 박았네.</td>\n",
       "      <td>잠도 못 자고 우는 아이 달래랴 수유하랴 머리카락이 다 빠지는 줄 알았어요.</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>네가 어른 노릇 부모 노릇을 하느라 고생이 정말 많구나.</td>\n",
       "      <td>그러면서 엄마 생각이 많이 났어요. 엄마는 직장까지 다니면서 나를 키우느라 얼마나 ...</td>\n",
       "      <td>기쁨</td>\n",
       "      <td>아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               previous_text  \\\n",
       "0                                        NaN   \n",
       "1               그렇지? 아기 키우는 게 여간 어려운 일이 아니야.   \n",
       "2    배가 고파서 그랬던 것 아닐까? 아기들은 배가 고프면 몸부림을 친단다.   \n",
       "3  말도 마. 네가 얼마나 까탈스러웠는데. 우리 손주가 내 딸을 빼다 박았네.   \n",
       "4            네가 어른 노릇 부모 노릇을 하느라 고생이 정말 많구나.   \n",
       "\n",
       "                                        current_text sentiments  \\\n",
       "0            엄마 아기가 태어나니까 내가 부모로서 해야 할 게 참 많은 것 같아요.         기쁨   \n",
       "1      어제 평소보다도 격하게 막 온몸을 써가면서 울더라고요. 얼마나 당황했는지 몰라요.         기쁨   \n",
       "2           맞아요. 젖을 물려주니 금세 뚝 그쳤어요. 난 분명히 순했을 것 같은데.         기쁨   \n",
       "3         잠도 못 자고 우는 아이 달래랴 수유하랴 머리카락이 다 빠지는 줄 알았어요.         기쁨   \n",
       "4  그러면서 엄마 생각이 많이 났어요. 엄마는 직장까지 다니면서 나를 키우느라 얼마나 ...         기쁨   \n",
       "\n",
       "                       situation  \n",
       "0  아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.  \n",
       "1  아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.  \n",
       "2  아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.  \n",
       "3  아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.  \n",
       "4  아이가 태어나니 부모가 배우고 익혀야 할 것이 많다.  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data1 = pd.read_csv('data1.csv',encoding='cp949')\n",
    "data2 = pd.read_csv('data2.csv',encoding='cp949')\n",
    "\n",
    "data1_test = pd.read_csv('data1_test.csv',encoding='cp949')\n",
    "data2_test = pd.read_csv('data2_test.csv',encoding='cp949')\n",
    "\n",
    "\n",
    "data1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define constants\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "\n",
    "# Sentiment mapping\n",
    "sentiment_map = {\n",
    "    '기쁨': 5,\n",
    "    '당황': 4,\n",
    "    '상처': 3,\n",
    "    '불안': 2,\n",
    "    '슬픔': 1,\n",
    "    '분노': 0\n",
    "}\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(sentiment_map))\n",
    "model.cuda()\n",
    "\n",
    "# Tokenizer and convert to vector function\n",
    "def tokenizer_and_convert_to_vector(text, tokenizer, max_len):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, max_length=max_len, truncation=True)\n",
    "    return pad_sequences([tokens], maxlen=max_len, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")[0]\n",
    "\n",
    "def prepare_data(data, tokenizer, max_len):\n",
    "    # Combine previous_text and current_text\n",
    "    combined_texts = data.apply(lambda row: f\"{row['current_text']}\", axis=1)\n",
    "    \n",
    "    # Tokenize using batch_encode_plus for faster processing\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        combined_texts.tolist(),  # Convert Series to list\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',  # Pad sequences to the maximum length\n",
    "        truncation=True,\n",
    "        return_tensors='pt'  # Directly return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Prepare input ids and attention masks\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_masks = tokens['attention_mask']\n",
    "    \n",
    "    # Map sentiments to integer labels\n",
    "    labels = data['sentiments'].apply(lambda x: sentiment_map.get(x, -1)).values\n",
    "    labels = torch.tensor(labels, dtype=torch.long)  # Convert to PyTorch tensor\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# 2. 데이터 준비 (prepare_data 이후)\n",
    "x_train, train_masks, y_train = prepare_data(data1, tokenizer, MAX_LEN)\n",
    "x_test, test_masks, y_test = prepare_data(data1_test, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(5)\n",
      "tensor(0)\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "print(torch.min(y_train))\n",
    "print(torch.max(y_train))\n",
    "print(torch.min(y_test))\n",
    "print(torch.max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoader for training and validation\n",
    "import random\n",
    "\n",
    "train_data = TensorDataset(x_train, train_masks, y_train)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(x_test, test_masks, y_test)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr =2e-5, eps = 1e-8)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_training_steps=0, num_warmup_steps = total_steps)\n",
    "\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    50  of  5,510.    Elapsed: 0:00:31.    Loss: 1.8049\n",
      "  Batch   100  of  5,510.    Elapsed: 0:01:17.    Loss: 1.8274\n",
      "  Batch   150  of  5,510.    Elapsed: 0:02:06.    Loss: 1.8056\n",
      "  Batch   200  of  5,510.    Elapsed: 0:02:56.    Loss: 1.7679\n",
      "  Batch   250  of  5,510.    Elapsed: 0:03:47.    Loss: 1.7957\n",
      "  Batch   300  of  5,510.    Elapsed: 0:04:39.    Loss: 1.7746\n",
      "  Batch   350  of  5,510.    Elapsed: 0:05:31.    Loss: 1.8250\n",
      "  Batch   400  of  5,510.    Elapsed: 0:06:23.    Loss: 1.7974\n",
      "  Batch   450  of  5,510.    Elapsed: 0:07:15.    Loss: 1.7929\n",
      "  Batch   500  of  5,510.    Elapsed: 0:08:07.    Loss: 1.7988\n",
      "  Batch   550  of  5,510.    Elapsed: 0:08:59.    Loss: 1.7642\n",
      "  Batch   600  of  5,510.    Elapsed: 0:09:52.    Loss: 1.7747\n",
      "  Batch   650  of  5,510.    Elapsed: 0:10:44.    Loss: 1.7694\n",
      "  Batch   700  of  5,510.    Elapsed: 0:11:37.    Loss: 1.7820\n",
      "  Batch   750  of  5,510.    Elapsed: 0:12:30.    Loss: 1.7725\n",
      "  Batch   800  of  5,510.    Elapsed: 0:13:22.    Loss: 1.7608\n",
      "  Batch   850  of  5,510.    Elapsed: 0:14:15.    Loss: 1.7899\n",
      "  Batch   900  of  5,510.    Elapsed: 0:15:07.    Loss: 1.8005\n",
      "  Batch   950  of  5,510.    Elapsed: 0:16:00.    Loss: 1.7480\n",
      "  Batch 1,000  of  5,510.    Elapsed: 0:16:52.    Loss: 1.7934\n",
      "  Batch 1,050  of  5,510.    Elapsed: 0:17:45.    Loss: 1.7594\n",
      "  Batch 1,100  of  5,510.    Elapsed: 0:18:37.    Loss: 1.7249\n",
      "  Batch 1,150  of  5,510.    Elapsed: 0:19:30.    Loss: 1.7144\n",
      "  Batch 1,200  of  5,510.    Elapsed: 0:20:22.    Loss: 1.7607\n",
      "  Batch 1,250  of  5,510.    Elapsed: 0:21:14.    Loss: 1.7858\n",
      "  Batch 1,300  of  5,510.    Elapsed: 0:22:07.    Loss: 1.7769\n",
      "  Batch 1,350  of  5,510.    Elapsed: 0:22:59.    Loss: 1.8079\n",
      "  Batch 1,400  of  5,510.    Elapsed: 0:23:51.    Loss: 1.7945\n",
      "  Batch 1,450  of  5,510.    Elapsed: 0:24:44.    Loss: 1.7548\n",
      "  Batch 1,500  of  5,510.    Elapsed: 0:25:37.    Loss: 1.7390\n",
      "  Batch 1,550  of  5,510.    Elapsed: 0:26:30.    Loss: 1.7963\n",
      "  Batch 1,600  of  5,510.    Elapsed: 0:27:23.    Loss: 1.7916\n",
      "  Batch 1,650  of  5,510.    Elapsed: 0:28:17.    Loss: 1.7785\n",
      "  Batch 1,700  of  5,510.    Elapsed: 0:29:10.    Loss: 1.7632\n",
      "  Batch 1,750  of  5,510.    Elapsed: 0:30:03.    Loss: 1.7676\n",
      "  Batch 1,800  of  5,510.    Elapsed: 0:30:56.    Loss: 1.7935\n",
      "  Batch 1,850  of  5,510.    Elapsed: 0:31:49.    Loss: 1.7959\n",
      "  Batch 1,900  of  5,510.    Elapsed: 0:32:43.    Loss: 1.7987\n",
      "  Batch 1,950  of  5,510.    Elapsed: 0:33:36.    Loss: 1.7247\n",
      "  Batch 2,000  of  5,510.    Elapsed: 0:34:28.    Loss: 1.7323\n",
      "  Batch 2,050  of  5,510.    Elapsed: 0:35:21.    Loss: 1.7052\n",
      "  Batch 2,100  of  5,510.    Elapsed: 0:36:14.    Loss: 1.6449\n",
      "  Batch 2,150  of  5,510.    Elapsed: 0:37:07.    Loss: 1.5952\n",
      "  Batch 2,200  of  5,510.    Elapsed: 0:38:00.    Loss: 1.7464\n",
      "  Batch 2,250  of  5,510.    Elapsed: 0:38:53.    Loss: 1.6992\n",
      "  Batch 2,300  of  5,510.    Elapsed: 0:39:46.    Loss: 1.6214\n",
      "  Batch 2,350  of  5,510.    Elapsed: 0:40:39.    Loss: 1.6843\n",
      "  Batch 2,400  of  5,510.    Elapsed: 0:41:32.    Loss: 1.6904\n",
      "  Batch 2,450  of  5,510.    Elapsed: 0:42:25.    Loss: 1.6149\n",
      "  Batch 2,500  of  5,510.    Elapsed: 0:43:18.    Loss: 1.5586\n",
      "  Batch 2,550  of  5,510.    Elapsed: 0:44:10.    Loss: 1.6767\n",
      "  Batch 2,600  of  5,510.    Elapsed: 0:45:03.    Loss: 1.5677\n",
      "  Batch 2,650  of  5,510.    Elapsed: 0:45:56.    Loss: 1.6539\n",
      "  Batch 2,700  of  5,510.    Elapsed: 0:46:48.    Loss: 1.6572\n",
      "  Batch 2,750  of  5,510.    Elapsed: 0:47:41.    Loss: 1.4807\n",
      "  Batch 2,800  of  5,510.    Elapsed: 0:48:34.    Loss: 1.4843\n",
      "  Batch 2,850  of  5,510.    Elapsed: 0:49:27.    Loss: 1.6121\n",
      "  Batch 2,900  of  5,510.    Elapsed: 0:50:19.    Loss: 1.5469\n",
      "  Batch 2,950  of  5,510.    Elapsed: 0:51:12.    Loss: 1.6239\n",
      "  Batch 3,000  of  5,510.    Elapsed: 0:52:04.    Loss: 1.6439\n",
      "  Batch 3,050  of  5,510.    Elapsed: 0:52:57.    Loss: 1.5025\n",
      "  Batch 3,100  of  5,510.    Elapsed: 0:53:49.    Loss: 1.6593\n",
      "  Batch 3,150  of  5,510.    Elapsed: 0:54:42.    Loss: 1.6555\n",
      "  Batch 3,200  of  5,510.    Elapsed: 0:55:34.    Loss: 1.6740\n",
      "  Batch 3,250  of  5,510.    Elapsed: 0:56:27.    Loss: 1.5675\n",
      "  Batch 3,300  of  5,510.    Elapsed: 0:57:19.    Loss: 1.5958\n",
      "  Batch 3,350  of  5,510.    Elapsed: 0:58:12.    Loss: 1.5225\n",
      "  Batch 3,400  of  5,510.    Elapsed: 0:59:05.    Loss: 1.4910\n",
      "  Batch 3,450  of  5,510.    Elapsed: 0:59:58.    Loss: 1.4998\n",
      "  Batch 3,500  of  5,510.    Elapsed: 1:00:51.    Loss: 1.6974\n",
      "  Batch 3,550  of  5,510.    Elapsed: 1:01:43.    Loss: 1.5436\n",
      "  Batch 3,600  of  5,510.    Elapsed: 1:02:36.    Loss: 1.4221\n",
      "  Batch 3,650  of  5,510.    Elapsed: 1:03:29.    Loss: 1.5714\n",
      "  Batch 3,700  of  5,510.    Elapsed: 1:04:21.    Loss: 1.4976\n",
      "  Batch 3,750  of  5,510.    Elapsed: 1:05:14.    Loss: 1.5163\n",
      "  Batch 3,800  of  5,510.    Elapsed: 1:06:06.    Loss: 1.4969\n",
      "  Batch 3,850  of  5,510.    Elapsed: 1:06:59.    Loss: 1.4156\n",
      "  Batch 3,900  of  5,510.    Elapsed: 1:07:52.    Loss: 1.4533\n",
      "  Batch 3,950  of  5,510.    Elapsed: 1:08:44.    Loss: 1.4762\n",
      "  Batch 4,000  of  5,510.    Elapsed: 1:09:37.    Loss: 1.6086\n",
      "  Batch 4,050  of  5,510.    Elapsed: 1:10:30.    Loss: 1.6979\n",
      "  Batch 4,100  of  5,510.    Elapsed: 1:11:22.    Loss: 1.4646\n",
      "  Batch 4,150  of  5,510.    Elapsed: 1:12:15.    Loss: 1.4608\n",
      "  Batch 4,200  of  5,510.    Elapsed: 1:13:08.    Loss: 1.6885\n",
      "  Batch 4,250  of  5,510.    Elapsed: 1:14:01.    Loss: 1.4695\n",
      "  Batch 4,300  of  5,510.    Elapsed: 1:14:54.    Loss: 1.4701\n",
      "  Batch 4,350  of  5,510.    Elapsed: 1:15:46.    Loss: 1.5009\n",
      "  Batch 4,400  of  5,510.    Elapsed: 1:16:39.    Loss: 1.3784\n",
      "  Batch 4,450  of  5,510.    Elapsed: 1:17:32.    Loss: 1.5693\n",
      "  Batch 4,500  of  5,510.    Elapsed: 1:18:24.    Loss: 1.4086\n",
      "  Batch 4,550  of  5,510.    Elapsed: 1:19:17.    Loss: 1.8278\n",
      "  Batch 4,600  of  5,510.    Elapsed: 1:20:09.    Loss: 1.3972\n",
      "  Batch 4,650  of  5,510.    Elapsed: 1:21:01.    Loss: 1.5358\n",
      "  Batch 4,700  of  5,510.    Elapsed: 1:21:54.    Loss: 1.4593\n",
      "  Batch 4,750  of  5,510.    Elapsed: 1:22:46.    Loss: 1.4555\n",
      "  Batch 4,800  of  5,510.    Elapsed: 1:23:38.    Loss: 1.5540\n",
      "  Batch 4,850  of  5,510.    Elapsed: 1:24:31.    Loss: 1.8192\n",
      "  Batch 4,900  of  5,510.    Elapsed: 1:25:24.    Loss: 1.3484\n",
      "  Batch 4,950  of  5,510.    Elapsed: 1:26:17.    Loss: 1.3506\n",
      "  Batch 5,000  of  5,510.    Elapsed: 1:27:09.    Loss: 1.5588\n",
      "  Batch 5,050  of  5,510.    Elapsed: 1:28:02.    Loss: 1.3266\n",
      "  Batch 5,100  of  5,510.    Elapsed: 1:28:54.    Loss: 1.4797\n",
      "  Batch 5,150  of  5,510.    Elapsed: 1:29:46.    Loss: 1.4140\n",
      "  Batch 5,200  of  5,510.    Elapsed: 1:30:29.    Loss: 1.2215\n",
      "  Batch 5,250  of  5,510.    Elapsed: 1:31:13.    Loss: 1.5755\n",
      "  Batch 5,300  of  5,510.    Elapsed: 1:31:56.    Loss: 1.4152\n",
      "  Batch 5,350  of  5,510.    Elapsed: 1:32:40.    Loss: 1.4168\n",
      "  Batch 5,400  of  5,510.    Elapsed: 1:33:24.    Loss: 1.3047\n",
      "  Batch 5,450  of  5,510.    Elapsed: 1:34:07.    Loss: 1.7320\n",
      "  Batch 5,500  of  5,510.    Elapsed: 1:34:51.    Loss: 1.4217\n",
      "\n",
      "  Average training loss: 1.65\n",
      "  Training epoch took: 1:34:59\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.46\n",
      "  Validation took: 0:03:10\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch    50  of  5,510.    Elapsed: 0:00:44.    Loss: 1.2264\n",
      "  Batch   100  of  5,510.    Elapsed: 0:01:27.    Loss: 1.6499\n",
      "  Batch   150  of  5,510.    Elapsed: 0:02:10.    Loss: 1.4580\n",
      "  Batch   200  of  5,510.    Elapsed: 0:02:54.    Loss: 1.5701\n",
      "  Batch   250  of  5,510.    Elapsed: 0:03:37.    Loss: 1.3732\n",
      "  Batch   300  of  5,510.    Elapsed: 0:04:21.    Loss: 1.4395\n",
      "  Batch   350  of  5,510.    Elapsed: 0:05:04.    Loss: 1.4593\n",
      "  Batch   400  of  5,510.    Elapsed: 0:05:48.    Loss: 1.5952\n",
      "  Batch   450  of  5,510.    Elapsed: 0:06:31.    Loss: 1.5195\n",
      "  Batch   500  of  5,510.    Elapsed: 0:07:14.    Loss: 1.4334\n",
      "  Batch   550  of  5,510.    Elapsed: 0:07:58.    Loss: 1.5382\n",
      "  Batch   600  of  5,510.    Elapsed: 0:08:41.    Loss: 1.6722\n",
      "  Batch   650  of  5,510.    Elapsed: 0:09:24.    Loss: 1.3005\n",
      "  Batch   700  of  5,510.    Elapsed: 0:10:08.    Loss: 1.5160\n",
      "  Batch   750  of  5,510.    Elapsed: 0:10:51.    Loss: 1.3127\n",
      "  Batch   800  of  5,510.    Elapsed: 0:11:35.    Loss: 1.5176\n",
      "  Batch   850  of  5,510.    Elapsed: 0:12:18.    Loss: 1.2871\n",
      "  Batch   900  of  5,510.    Elapsed: 0:13:05.    Loss: 1.5324\n",
      "  Batch   950  of  5,510.    Elapsed: 0:14:00.    Loss: 1.4588\n",
      "  Batch 1,000  of  5,510.    Elapsed: 0:14:53.    Loss: 1.2938\n",
      "  Batch 1,050  of  5,510.    Elapsed: 0:15:46.    Loss: 1.4602\n",
      "  Batch 1,100  of  5,510.    Elapsed: 0:16:40.    Loss: 1.4275\n",
      "  Batch 1,150  of  5,510.    Elapsed: 0:17:34.    Loss: 1.6045\n",
      "  Batch 1,200  of  5,510.    Elapsed: 0:18:26.    Loss: 1.3856\n",
      "  Batch 1,250  of  5,510.    Elapsed: 0:19:19.    Loss: 1.5038\n",
      "  Batch 1,300  of  5,510.    Elapsed: 0:20:12.    Loss: 1.3339\n",
      "  Batch 1,350  of  5,510.    Elapsed: 0:21:05.    Loss: 1.4417\n",
      "  Batch 1,400  of  5,510.    Elapsed: 0:21:58.    Loss: 1.5707\n",
      "  Batch 1,450  of  5,510.    Elapsed: 0:22:51.    Loss: 1.4708\n",
      "  Batch 1,500  of  5,510.    Elapsed: 0:23:44.    Loss: 1.4588\n",
      "  Batch 1,550  of  5,510.    Elapsed: 0:24:37.    Loss: 1.4787\n",
      "  Batch 1,600  of  5,510.    Elapsed: 0:25:30.    Loss: 1.2682\n",
      "  Batch 1,650  of  5,510.    Elapsed: 0:26:22.    Loss: 1.6019\n",
      "  Batch 1,700  of  5,510.    Elapsed: 0:27:15.    Loss: 1.3337\n",
      "  Batch 1,750  of  5,510.    Elapsed: 0:28:07.    Loss: 1.1860\n",
      "  Batch 1,800  of  5,510.    Elapsed: 0:29:00.    Loss: 1.1521\n",
      "  Batch 1,850  of  5,510.    Elapsed: 0:29:53.    Loss: 1.4533\n",
      "  Batch 1,900  of  5,510.    Elapsed: 0:30:46.    Loss: 1.5775\n",
      "  Batch 1,950  of  5,510.    Elapsed: 0:31:39.    Loss: 1.3477\n",
      "  Batch 2,000  of  5,510.    Elapsed: 0:32:32.    Loss: 1.5266\n",
      "  Batch 2,050  of  5,510.    Elapsed: 0:33:25.    Loss: 1.1841\n",
      "  Batch 2,100  of  5,510.    Elapsed: 0:34:17.    Loss: 1.4737\n",
      "  Batch 2,150  of  5,510.    Elapsed: 0:35:10.    Loss: 1.5887\n",
      "  Batch 2,200  of  5,510.    Elapsed: 0:36:03.    Loss: 1.4173\n",
      "  Batch 2,250  of  5,510.    Elapsed: 0:36:56.    Loss: 1.5534\n",
      "  Batch 2,300  of  5,510.    Elapsed: 0:37:48.    Loss: 1.5604\n",
      "  Batch 2,350  of  5,510.    Elapsed: 0:38:41.    Loss: 1.5175\n",
      "  Batch 2,400  of  5,510.    Elapsed: 0:39:34.    Loss: 1.5191\n",
      "  Batch 2,450  of  5,510.    Elapsed: 0:40:27.    Loss: 1.3935\n",
      "  Batch 2,500  of  5,510.    Elapsed: 0:41:20.    Loss: 1.2752\n",
      "  Batch 2,550  of  5,510.    Elapsed: 0:42:13.    Loss: 1.3708\n",
      "  Batch 2,600  of  5,510.    Elapsed: 0:43:05.    Loss: 1.5310\n",
      "  Batch 2,650  of  5,510.    Elapsed: 0:43:58.    Loss: 1.3287\n",
      "  Batch 2,700  of  5,510.    Elapsed: 0:44:50.    Loss: 1.3527\n",
      "  Batch 2,750  of  5,510.    Elapsed: 0:45:43.    Loss: 1.3359\n",
      "  Batch 2,800  of  5,510.    Elapsed: 0:46:35.    Loss: 1.3757\n",
      "  Batch 2,850  of  5,510.    Elapsed: 0:47:28.    Loss: 1.4995\n",
      "  Batch 2,900  of  5,510.    Elapsed: 0:48:20.    Loss: 1.5187\n",
      "  Batch 2,950  of  5,510.    Elapsed: 0:49:13.    Loss: 1.3179\n",
      "  Batch 3,000  of  5,510.    Elapsed: 0:50:05.    Loss: 1.1686\n",
      "  Batch 3,050  of  5,510.    Elapsed: 0:50:58.    Loss: 1.3137\n",
      "  Batch 3,100  of  5,510.    Elapsed: 0:51:50.    Loss: 1.4768\n",
      "  Batch 3,150  of  5,510.    Elapsed: 0:52:42.    Loss: 1.1170\n",
      "  Batch 3,200  of  5,510.    Elapsed: 0:53:35.    Loss: 1.3444\n",
      "  Batch 3,250  of  5,510.    Elapsed: 0:54:27.    Loss: 1.0111\n",
      "  Batch 3,300  of  5,510.    Elapsed: 0:55:19.    Loss: 1.5644\n",
      "  Batch 3,350  of  5,510.    Elapsed: 0:56:11.    Loss: 1.5335\n",
      "  Batch 3,400  of  5,510.    Elapsed: 0:57:03.    Loss: 1.2115\n",
      "  Batch 3,450  of  5,510.    Elapsed: 0:57:55.    Loss: 1.4599\n",
      "  Batch 3,500  of  5,510.    Elapsed: 0:58:42.    Loss: 1.3071\n",
      "  Batch 3,550  of  5,510.    Elapsed: 0:59:25.    Loss: 1.2819\n",
      "  Batch 3,600  of  5,510.    Elapsed: 1:00:08.    Loss: 1.1450\n",
      "  Batch 3,650  of  5,510.    Elapsed: 1:00:51.    Loss: 1.1118\n",
      "  Batch 3,700  of  5,510.    Elapsed: 1:01:34.    Loss: 1.3948\n",
      "  Batch 3,750  of  5,510.    Elapsed: 1:02:19.    Loss: 1.3574\n",
      "  Batch 3,800  of  5,510.    Elapsed: 1:03:10.    Loss: 1.4081\n",
      "  Batch 3,850  of  5,510.    Elapsed: 1:04:02.    Loss: 1.3278\n",
      "  Batch 3,900  of  5,510.    Elapsed: 1:04:53.    Loss: 1.4540\n",
      "  Batch 3,950  of  5,510.    Elapsed: 1:05:45.    Loss: 1.4216\n",
      "  Batch 4,000  of  5,510.    Elapsed: 1:06:37.    Loss: 1.1974\n",
      "  Batch 4,050  of  5,510.    Elapsed: 1:07:29.    Loss: 1.3059\n",
      "  Batch 4,100  of  5,510.    Elapsed: 1:08:21.    Loss: 1.4137\n",
      "  Batch 4,150  of  5,510.    Elapsed: 1:09:12.    Loss: 1.4923\n",
      "  Batch 4,200  of  5,510.    Elapsed: 1:10:04.    Loss: 1.3387\n",
      "  Batch 4,250  of  5,510.    Elapsed: 1:10:56.    Loss: 1.8257\n",
      "  Batch 4,300  of  5,510.    Elapsed: 1:11:48.    Loss: 1.1309\n",
      "  Batch 4,350  of  5,510.    Elapsed: 1:12:40.    Loss: 1.2145\n",
      "  Batch 4,400  of  5,510.    Elapsed: 1:13:31.    Loss: 1.3631\n",
      "  Batch 4,450  of  5,510.    Elapsed: 1:14:23.    Loss: 1.2746\n",
      "  Batch 4,500  of  5,510.    Elapsed: 1:15:15.    Loss: 1.1623\n",
      "  Batch 4,550  of  5,510.    Elapsed: 1:16:06.    Loss: 1.2927\n",
      "  Batch 4,600  of  5,510.    Elapsed: 1:16:58.    Loss: 1.4159\n",
      "  Batch 4,650  of  5,510.    Elapsed: 1:17:50.    Loss: 1.4576\n",
      "  Batch 4,700  of  5,510.    Elapsed: 1:18:42.    Loss: 1.4322\n",
      "  Batch 4,750  of  5,510.    Elapsed: 1:19:34.    Loss: 1.7323\n",
      "  Batch 4,800  of  5,510.    Elapsed: 1:20:25.    Loss: 1.2706\n",
      "  Batch 4,850  of  5,510.    Elapsed: 1:21:17.    Loss: 1.4010\n",
      "  Batch 4,900  of  5,510.    Elapsed: 1:22:09.    Loss: 1.4455\n",
      "  Batch 4,950  of  5,510.    Elapsed: 1:23:01.    Loss: 1.3665\n",
      "  Batch 5,000  of  5,510.    Elapsed: 1:23:53.    Loss: 1.1802\n",
      "  Batch 5,050  of  5,510.    Elapsed: 1:24:45.    Loss: 1.4682\n",
      "  Batch 5,100  of  5,510.    Elapsed: 1:25:37.    Loss: 1.6533\n",
      "  Batch 5,150  of  5,510.    Elapsed: 1:26:29.    Loss: 1.5842\n",
      "  Batch 5,200  of  5,510.    Elapsed: 1:27:20.    Loss: 1.5604\n",
      "  Batch 5,250  of  5,510.    Elapsed: 1:28:12.    Loss: 1.7041\n",
      "  Batch 5,300  of  5,510.    Elapsed: 1:29:04.    Loss: 1.1357\n",
      "  Batch 5,350  of  5,510.    Elapsed: 1:29:56.    Loss: 1.1575\n",
      "  Batch 5,400  of  5,510.    Elapsed: 1:30:48.    Loss: 1.7325\n",
      "  Batch 5,450  of  5,510.    Elapsed: 1:31:40.    Loss: 1.3494\n",
      "  Batch 5,500  of  5,510.    Elapsed: 1:32:32.    Loss: 1.3520\n",
      "\n",
      "  Average training loss: 1.39\n",
      "  Training epoch took: 1:32:42\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.50\n",
      "  Validation took: 0:03:45\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch    50  of  5,510.    Elapsed: 0:00:52.    Loss: 1.3611\n",
      "  Batch   100  of  5,510.    Elapsed: 0:01:44.    Loss: 1.4344\n",
      "  Batch   150  of  5,510.    Elapsed: 0:02:36.    Loss: 1.4052\n",
      "  Batch   200  of  5,510.    Elapsed: 0:03:29.    Loss: 1.5924\n",
      "  Batch   250  of  5,510.    Elapsed: 0:04:20.    Loss: 1.2899\n",
      "  Batch   300  of  5,510.    Elapsed: 0:05:11.    Loss: 1.4318\n",
      "  Batch   350  of  5,510.    Elapsed: 0:06:00.    Loss: 1.5192\n",
      "  Batch   400  of  5,510.    Elapsed: 0:06:50.    Loss: 1.2643\n",
      "  Batch   450  of  5,510.    Elapsed: 0:07:39.    Loss: 1.3677\n",
      "  Batch   500  of  5,510.    Elapsed: 0:08:29.    Loss: 1.3487\n",
      "  Batch   550  of  5,510.    Elapsed: 0:09:19.    Loss: 1.1394\n",
      "  Batch   600  of  5,510.    Elapsed: 0:10:08.    Loss: 1.3495\n",
      "  Batch   650  of  5,510.    Elapsed: 0:10:58.    Loss: 1.4094\n",
      "  Batch   700  of  5,510.    Elapsed: 0:11:47.    Loss: 1.5294\n",
      "  Batch   750  of  5,510.    Elapsed: 0:12:36.    Loss: 1.1894\n",
      "  Batch   800  of  5,510.    Elapsed: 0:13:26.    Loss: 1.3567\n",
      "  Batch   850  of  5,510.    Elapsed: 0:14:15.    Loss: 1.3510\n",
      "  Batch   900  of  5,510.    Elapsed: 0:15:04.    Loss: 1.3492\n",
      "  Batch   950  of  5,510.    Elapsed: 0:15:53.    Loss: 1.5622\n",
      "  Batch 1,000  of  5,510.    Elapsed: 0:16:43.    Loss: 1.2830\n",
      "  Batch 1,050  of  5,510.    Elapsed: 0:17:32.    Loss: 1.2217\n",
      "  Batch 1,100  of  5,510.    Elapsed: 0:18:22.    Loss: 1.0763\n",
      "  Batch 1,150  of  5,510.    Elapsed: 0:19:12.    Loss: 1.4905\n",
      "  Batch 1,200  of  5,510.    Elapsed: 0:20:02.    Loss: 1.1837\n",
      "  Batch 1,250  of  5,510.    Elapsed: 0:20:51.    Loss: 1.4502\n",
      "  Batch 1,300  of  5,510.    Elapsed: 0:21:41.    Loss: 1.3111\n",
      "  Batch 1,350  of  5,510.    Elapsed: 0:22:31.    Loss: 1.5270\n",
      "  Batch 1,400  of  5,510.    Elapsed: 0:23:21.    Loss: 1.2447\n",
      "  Batch 1,450  of  5,510.    Elapsed: 0:24:10.    Loss: 1.2920\n",
      "  Batch 1,500  of  5,510.    Elapsed: 0:25:00.    Loss: 1.4566\n",
      "  Batch 1,550  of  5,510.    Elapsed: 0:25:50.    Loss: 1.4390\n",
      "  Batch 1,600  of  5,510.    Elapsed: 0:26:39.    Loss: 1.4237\n",
      "  Batch 1,650  of  5,510.    Elapsed: 0:27:28.    Loss: 1.4144\n",
      "  Batch 1,700  of  5,510.    Elapsed: 0:28:18.    Loss: 1.3685\n",
      "  Batch 1,750  of  5,510.    Elapsed: 0:29:07.    Loss: 1.0134\n",
      "  Batch 1,800  of  5,510.    Elapsed: 0:29:57.    Loss: 1.2414\n",
      "  Batch 1,850  of  5,510.    Elapsed: 0:30:46.    Loss: 1.1665\n",
      "  Batch 1,900  of  5,510.    Elapsed: 0:31:36.    Loss: 1.3603\n",
      "  Batch 1,950  of  5,510.    Elapsed: 0:32:26.    Loss: 1.4570\n",
      "  Batch 2,000  of  5,510.    Elapsed: 0:33:15.    Loss: 1.2450\n",
      "  Batch 2,050  of  5,510.    Elapsed: 0:34:04.    Loss: 1.1332\n",
      "  Batch 2,100  of  5,510.    Elapsed: 0:34:54.    Loss: 1.3982\n",
      "  Batch 2,150  of  5,510.    Elapsed: 0:35:43.    Loss: 1.4864\n",
      "  Batch 2,200  of  5,510.    Elapsed: 0:36:33.    Loss: 1.4650\n",
      "  Batch 2,250  of  5,510.    Elapsed: 0:37:22.    Loss: 1.0921\n",
      "  Batch 2,300  of  5,510.    Elapsed: 0:38:12.    Loss: 1.4770\n",
      "  Batch 2,350  of  5,510.    Elapsed: 0:39:02.    Loss: 1.2556\n",
      "  Batch 2,400  of  5,510.    Elapsed: 0:39:51.    Loss: 1.5169\n",
      "  Batch 2,450  of  5,510.    Elapsed: 0:40:41.    Loss: 1.2686\n",
      "  Batch 2,500  of  5,510.    Elapsed: 0:41:30.    Loss: 1.4470\n",
      "  Batch 2,550  of  5,510.    Elapsed: 0:42:20.    Loss: 1.4001\n",
      "  Batch 2,600  of  5,510.    Elapsed: 0:43:09.    Loss: 1.3212\n",
      "  Batch 2,650  of  5,510.    Elapsed: 0:43:59.    Loss: 1.3433\n",
      "  Batch 2,700  of  5,510.    Elapsed: 0:44:48.    Loss: 1.3030\n",
      "  Batch 2,750  of  5,510.    Elapsed: 0:45:38.    Loss: 1.2433\n",
      "  Batch 2,800  of  5,510.    Elapsed: 0:46:27.    Loss: 1.2580\n",
      "  Batch 2,850  of  5,510.    Elapsed: 0:47:17.    Loss: 1.3802\n",
      "  Batch 2,900  of  5,510.    Elapsed: 0:48:06.    Loss: 1.2126\n",
      "  Batch 2,950  of  5,510.    Elapsed: 0:48:56.    Loss: 1.4536\n",
      "  Batch 3,000  of  5,510.    Elapsed: 0:49:45.    Loss: 1.4345\n",
      "  Batch 3,050  of  5,510.    Elapsed: 0:50:35.    Loss: 1.3049\n",
      "  Batch 3,100  of  5,510.    Elapsed: 0:51:24.    Loss: 1.0886\n",
      "  Batch 3,150  of  5,510.    Elapsed: 0:52:14.    Loss: 1.2642\n",
      "  Batch 3,200  of  5,510.    Elapsed: 0:53:03.    Loss: 1.1804\n",
      "  Batch 3,250  of  5,510.    Elapsed: 0:53:53.    Loss: 1.3475\n",
      "  Batch 3,300  of  5,510.    Elapsed: 0:54:42.    Loss: 1.2397\n",
      "  Batch 3,350  of  5,510.    Elapsed: 0:55:32.    Loss: 1.1657\n",
      "  Batch 3,400  of  5,510.    Elapsed: 0:56:21.    Loss: 1.1100\n",
      "  Batch 3,450  of  5,510.    Elapsed: 0:57:11.    Loss: 1.4518\n",
      "  Batch 3,500  of  5,510.    Elapsed: 0:58:02.    Loss: 1.0499\n",
      "  Batch 3,550  of  5,510.    Elapsed: 0:58:53.    Loss: 1.0999\n",
      "  Batch 3,600  of  5,510.    Elapsed: 0:59:44.    Loss: 1.5857\n",
      "  Batch 3,650  of  5,510.    Elapsed: 1:00:38.    Loss: 1.0774\n",
      "  Batch 3,700  of  5,510.    Elapsed: 1:01:31.    Loss: 1.1743\n",
      "  Batch 3,750  of  5,510.    Elapsed: 1:02:24.    Loss: 1.3024\n",
      "  Batch 3,800  of  5,510.    Elapsed: 1:03:16.    Loss: 1.2533\n",
      "  Batch 3,850  of  5,510.    Elapsed: 1:04:08.    Loss: 0.9640\n",
      "  Batch 3,900  of  5,510.    Elapsed: 1:05:00.    Loss: 1.3284\n",
      "  Batch 3,950  of  5,510.    Elapsed: 1:05:52.    Loss: 1.2820\n",
      "  Batch 4,000  of  5,510.    Elapsed: 1:06:43.    Loss: 1.4867\n",
      "  Batch 4,050  of  5,510.    Elapsed: 1:07:35.    Loss: 1.2073\n",
      "  Batch 4,100  of  5,510.    Elapsed: 1:08:27.    Loss: 1.1519\n",
      "  Batch 4,150  of  5,510.    Elapsed: 1:09:19.    Loss: 1.1704\n",
      "  Batch 4,200  of  5,510.    Elapsed: 1:10:11.    Loss: 1.0819\n",
      "  Batch 4,250  of  5,510.    Elapsed: 1:11:03.    Loss: 1.4106\n",
      "  Batch 4,300  of  5,510.    Elapsed: 1:11:55.    Loss: 1.4957\n",
      "  Batch 4,350  of  5,510.    Elapsed: 1:12:47.    Loss: 1.2285\n",
      "  Batch 4,400  of  5,510.    Elapsed: 1:13:39.    Loss: 1.4992\n",
      "  Batch 4,450  of  5,510.    Elapsed: 1:14:30.    Loss: 1.1260\n",
      "  Batch 4,500  of  5,510.    Elapsed: 1:15:23.    Loss: 0.9893\n",
      "  Batch 4,550  of  5,510.    Elapsed: 1:16:15.    Loss: 1.4221\n",
      "  Batch 4,600  of  5,510.    Elapsed: 1:17:07.    Loss: 1.3389\n",
      "  Batch 4,650  of  5,510.    Elapsed: 1:17:59.    Loss: 1.3605\n",
      "  Batch 4,700  of  5,510.    Elapsed: 1:18:51.    Loss: 1.3176\n",
      "  Batch 4,750  of  5,510.    Elapsed: 1:19:43.    Loss: 1.6385\n",
      "  Batch 4,800  of  5,510.    Elapsed: 1:20:35.    Loss: 1.4182\n",
      "  Batch 4,850  of  5,510.    Elapsed: 1:21:27.    Loss: 1.1163\n",
      "  Batch 4,900  of  5,510.    Elapsed: 1:22:19.    Loss: 1.5435\n",
      "  Batch 4,950  of  5,510.    Elapsed: 1:23:11.    Loss: 1.2458\n",
      "  Batch 5,000  of  5,510.    Elapsed: 1:24:03.    Loss: 1.3576\n",
      "  Batch 5,050  of  5,510.    Elapsed: 1:24:55.    Loss: 1.3547\n",
      "  Batch 5,100  of  5,510.    Elapsed: 1:25:47.    Loss: 1.0956\n",
      "  Batch 5,150  of  5,510.    Elapsed: 1:26:39.    Loss: 1.1937\n",
      "  Batch 5,200  of  5,510.    Elapsed: 1:27:31.    Loss: 1.0899\n",
      "  Batch 5,250  of  5,510.    Elapsed: 1:28:23.    Loss: 1.2879\n",
      "  Batch 5,300  of  5,510.    Elapsed: 1:29:14.    Loss: 1.2911\n",
      "  Batch 5,350  of  5,510.    Elapsed: 1:30:06.    Loss: 1.3921\n",
      "  Batch 5,400  of  5,510.    Elapsed: 1:30:59.    Loss: 1.2460\n",
      "  Batch 5,450  of  5,510.    Elapsed: 1:31:50.    Loss: 1.3220\n",
      "  Batch 5,500  of  5,510.    Elapsed: 1:32:42.    Loss: 1.1681\n",
      "\n",
      "  Average training loss: 1.33\n",
      "  Training epoch took: 1:32:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.51\n",
      "  Validation took: 0:03:40\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# 정확도 계산 함수\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# 시간 표시 함수\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Early Stopping 설정\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience  # 개선이 없을 경우 기다리는 에폭 수\n",
    "        self.min_delta = min_delta  # 최소 변화량\n",
    "        self.counter = 0  # 개선이 없을 때마다 증가\n",
    "        self.best_loss = None  # 최저 손실\n",
    "        self.early_stop = False  # 멈출지 여부\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# EarlyStopping 인스턴스 생성\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# 그래디언트 초기화\n",
    "model.zero_grad()\n",
    "\n",
    "# 에폭만큼 반복\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n",
    "    print('Training...')\n",
    "\n",
    "    # 시작 시간 설정\n",
    "    t0 = time.time()\n",
    "\n",
    "    # 로스 초기화\n",
    "    total_loss = 0\n",
    "\n",
    "    # 훈련모드로 변경\n",
    "    model.train()\n",
    "        \n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 경과 정보 표시\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Loss: {:.4f}'.format(step, len(train_dataloader), elapsed, loss.item()))\n",
    "\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        # Forward 수행\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        # 로스 구함\n",
    "        loss = outputs[0]\n",
    "\n",
    "        # 총 로스 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward 수행으로 그래디언트 계산\n",
    "        loss.backward()\n",
    "\n",
    "        # 그래디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 스케줄러로 학습률 감소\n",
    "        scheduler.step()\n",
    "\n",
    "        # 그래디언트 초기화\n",
    "        model.zero_grad()\n",
    "\n",
    "    # 평균 로스 계산\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    eval_loss = 0\n",
    "    eval_accuracy = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():     \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    avg_val_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # Early Stopping 적용\n",
    "    early_stopping(eval_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping applied!\")\n",
    "        break\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하기\n",
    "torch.save(model.state_dict(), 'roberta_sentiment_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 확률 순위별 출력:\n",
      "기쁨 (percentage: 94.35)\n",
      "슬픔 (percentage: 1.66)\n",
      "분노 (percentage: 1.58)\n",
      "불안 (percentage: 1.22)\n",
      "당황 (percentage: 0.73)\n",
      "상처 (percentage: 0.45)\n"
     ]
    }
   ],
   "source": [
    "# 예측 및 확률 출력 함수\n",
    "def predict_with_probabilities(text, model, tokenizer, device, max_len=128):\n",
    "    # 모델을 평가모드로 설정\n",
    "    model.eval()\n",
    "    \n",
    "    # 입력 텍스트를 토큰화 및 벡터화\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # 예측 수행\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # 로짓(logit)에서 소프트맥스 함수 적용하여 확률로 변환\n",
    "    logits = outputs[0]\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1).flatten()\n",
    "\n",
    "    # 확률을 내림차순으로 정렬하고 순위별로 클래스와 확률 출력\n",
    "    sorted_indices = torch.argsort(probabilities, descending=True)\n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "\n",
    "\n",
    "    \n",
    "    sentiment_map = {\n",
    "        5: '기쁨',\n",
    "        4: '당황',\n",
    "        3: '상처',\n",
    "        2: '불안',\n",
    "        1: '슬픔',\n",
    "        0: '분노'\n",
    "    }\n",
    "    print(\"예측 확률 순위별 출력:\")\n",
    "    for idx, prob in zip(sorted_indices, sorted_probabilities):\n",
    "        sentiment = sentiment_map[idx.item()]\n",
    "        print(f\"{sentiment} ({probabilities[idx].item() * 100:.2f} %)\")\n",
    "    \n",
    "    return sorted_indices, sorted_probabilities\n",
    "\n",
    "# 예측 함수 사용 예시\n",
    "text = \"오늘은 기분이 정말 좋아요!\"\n",
    "sorted_indices, sorted_probabilities = predict_with_probabilities(text, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create attention masks\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# attention_masks = []\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# for seq in x_train:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create attention masks using vectorized operations\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m x_train_cpu \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(x_train_cpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     19\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(attention_masks, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "\n",
    "# Create attention masks\n",
    "# attention_masks = []\n",
    "# for seq in x_train:\n",
    "#     print(f\"\")\n",
    "#     att_mask = [int(token_id > 0) for token_id in seq]\n",
    "#     attention_masks.append(att_mask)\n",
    "\n",
    "# Create attention masks using vectorized operations\n",
    "x_train_cpu = x_train.cpu()\n",
    "attention_masks = np.where(x_train_cpu > 0, 1, 0)\n",
    "attention_masks = torch.tensor(attention_masks, dtype=torch.long).to(device)\n",
    "\n",
    "# Convert attention masks to torch tensor\n",
    "    \n",
    "    \n",
    "# Convert to torch tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.long)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(x_train, y_train, random_state=42, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, x_train, random_state=42, test_size=0.1)\n",
    "\n",
    "# Create DataLoader for training and validation\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * 3  # Assuming 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Print a sample to verify\n",
    "print(\"Sample x_train:\", x_train[0])\n",
    "print(\"Sample y_train:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[0;32m    461\u001b[0m     )\n\u001b[0;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[1;34m(self, tensor_contents)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[0;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[1;34m(inp, tensor_contents)\u001b[0m\n\u001b[0;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[0;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[0;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[0;32m    348\u001b[0m     )\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor_str.py:134\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[1;32m--> 134\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\_tensor.py:986\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\임석범\\AppData\\Local\\Temp\\ipykernel_12772\\2094319600.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2480 [00:00<?, ?it/s]C:\\Users\\임석범\\AppData\\Local\\Temp\\ipykernel_12772\\2094319600.py:24: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "c:\\Users\\임석범\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:370: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Training:  66%|██████▋   | 1643/2480 [16:32<08:25,  1.65it/s, loss=1.75]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 78\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_val_accuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, validation_dataloader, optimizer, scheduler, device, epochs, validation_frequency)\u001b[0m\n\u001b[0;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Accumulate the loss\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Backward pass with mixed precision\u001b[39;00m\n\u001b[0;32m     32\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training function with progress printing and mixed precision training\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_dataloader, validation_dataloader, optimizer, scheduler, device, epochs=EPOCHS, validation_frequency=1):\n",
    "    model.to(device)  # Move model to GPU\n",
    "    scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\")\n",
    "\n",
    "        for step, batch in progress_bar:\n",
    "            # Unpack the batch and load onto device\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero the gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward pass with mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            # Accumulate the loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass with mixed precision\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to avoid exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and learning rate\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        # Calculate the average loss for this epoch\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Validation (run every `validation_frequency` epochs)\n",
    "        if (epoch + 1) % validation_frequency == 0:\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            eval_accuracy = 0\n",
    "            \n",
    "            print(f\"Running Validation... (Epoch {epoch + 1}/{epochs})\")\n",
    "            for batch in tqdm(validation_dataloader, desc=\"Validation\"):\n",
    "                # Unpack the batch and load onto device\n",
    "                b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                # No gradient calculation during evaluation\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                # Accumulate validation loss\n",
    "                eval_loss += loss.item()\n",
    "\n",
    "                # Get the predictions and calculate accuracy\n",
    "                preds = torch.argmax(logits, dim=1).flatten()\n",
    "                eval_accuracy += (preds == b_labels).cpu().numpy().mean()\n",
    "\n",
    "            # Calculate the average accuracy for this epoch\n",
    "            avg_val_accuracy = eval_accuracy / len(validation_dataloader)\n",
    "            print(f\"Validation Accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, validation_dataloader, optimizer, scheduler, device, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
